<p align="center">
  <img src="docs/assets/omniplay-logo.png" alt="OmniPlay Logo" width="280"/>
</p>

<h1 align="center">ğŸ® OmniPlay Benchmark</h1>

<p align="center">
  <b>The First Diagnostic Benchmark for Omni-Modal Agentic Reasoning</b>
</p>

<p align="center">
  <a href="https://arxiv.org/abs/2508.04361"><img src="https://img.shields.io/badge/ğŸ“„_Paper-arXiv_2508.04361-B31B1B.svg?style=for-the-badge" alt="arXiv"></a>
  <a href="#-leaderboard"><img src="https://img.shields.io/badge/ğŸ†_Leaderboard-View_Results-4285F4.svg?style=for-the-badge" alt="Leaderboard"></a>
  <a href="LICENSE"><img src="https://img.shields.io/badge/ğŸ“œ_License-MIT-00C851.svg?style=for-the-badge" alt="License"></a>
</p>

<p align="center">
  <img src="https://img.shields.io/badge/Python-3.8+-3776AB?style=flat-square&logo=python&logoColor=white" alt="Python">
  <img src="https://img.shields.io/badge/PyTorch-Compatible-EE4C2C?style=flat-square&logo=pytorch&logoColor=white" alt="PyTorch">
  <img src="https://img.shields.io/badge/Gymnasium-Supported-232F3E?style=flat-square" alt="Gymnasium">
  <img src="https://img.shields.io/badge/ğŸ…_SOTA-Gemini_2.5_Pro-FFD700?style=flat-square" alt="SOTA">
</p>

<p align="center">
  <a href="#-demo-showcase">Demo</a> â€¢
  <a href="#-quick-start">Quick Start</a> â€¢
  <a href="#-games">Games</a> â€¢
  <a href="#-unified-framework">Framework</a> â€¢
  <a href="#-leaderboard">Leaderboard</a> â€¢
  <a href="#-citation">Citation</a>
</p>

---

## ğŸ¬ Demo Showcase

<p align="center">
  <i>Watch AI models navigate, strategize, and reason across multiple modalities in real-time</i>
</p>

<table align="center">
  <tr>
    <td align="center" width="50%">
      <img src="docs/assets/demos/whispered_pathfinding.gif" width="100%" alt="Whispered Pathfinding"/>
      <br/>
      <b>ğŸ”Š Whispered Pathfinding</b>
      <br/>
      <i>3D maze navigation via voice guidance</i>
    </td>
    <td align="center" width="50%">
      <img src="docs/assets/demos/blasting_showdown.gif" width="100%" alt="Blasting Showdown"/>
      <br/>
      <b>ğŸ’£ Blasting Showdown</b>
      <br/>
      <i>Multi-agent Bomberman battles</i>
    </td>
  </tr>
  <tr>
    <td align="center" width="50%">
      <img src="docs/assets/demos/myriad_echoes.gif" width="100%" alt="Myriad Echoes"/>
      <br/>
      <b>ğŸµ Myriad Echoes</b>
      <br/>
      <i>Audiovisual sequence memory</i>
    </td>
    <td align="center" width="50%">
      <img src="docs/assets/demos/phantom_soldiers.gif" width="100%" alt="Phantom Soldiers"/>
      <br/>
      <b>ğŸ‘» Phantom Soldiers</b>
      <br/>
      <i>Tactical fog-of-war command</i>
    </td>
  </tr>
</table>

---

## ğŸ“– Overview

**OmniPlay** is the first comprehensive benchmark for evaluating omni-modal AI models through interactive game environments. We test how well models can process **ğŸ¬ Video**, **ğŸ”Š Audio**, **ğŸ–¼ï¸ Image**, and **ğŸ“ Text** simultaneously in dynamic scenarios â€” not just understand them in isolation, but **reason across all modalities to take actions**.

<p align="center">
  <table>
    <tr>
      <td align="center">ğŸ®<br/><b>5 Games</b></td>
      <td align="center">ğŸ¥<br/><b>Video Input</b></td>
      <td align="center">ğŸ”Š<br/><b>Audio Input</b></td>
      <td align="center">ğŸ§ <br/><b>Agentic Reasoning</b></td>
      <td align="center">ğŸ“Š<br/><b>Human Baseline</b></td>
    </tr>
  </table>
</p>

### Why OmniPlay?

| Feature | Description |
|:--------|:------------|
| ğŸ® **Interactive** | Models must take actions in real-time game environments, not just answer questions |
| ğŸŒ **Truly Omni-Modal** | Tests video, audio, image, and text understanding **together** |
| ğŸ§  **Agentic Reasoning** | Games demand cross-modal reasoning, planning, and decision-making |
| ğŸ¯ **Diagnostic** | Reveals specific capability gaps through diverse task types |
| ğŸ“ˆ **Comparable** | Standardized metrics with human expert baselines |

### âœ¨ Key Findings

> ğŸ”¬ **Capability Schism**: Current omni-modal models show severe performance gaps across tasks. Gemini 2.5 Pro achieves **97.5%** on audio navigation but only **28.4%** on audio-visual learning â€” a **3.4x** difference!

---

## ğŸ® Games

| Game | Modalities | Task | Key Skills Evaluated |
|:-----|:-----------|:-----|:---------------------|
| **ğŸ§ª The Alchemist's Melody** | ğŸ–¼ï¸ Image + ğŸ”Š Audio + ğŸ“ State | Learn color-note mappings to reproduce sequences | Audio-visual association, pattern learning |
| **ğŸµ Myriad Echoes** | ğŸ¬ Video + ğŸ”Š Audio + ğŸ–¼ï¸ Image | Observe and reproduce audiovisual sequences | Sequence memory, coordinate prediction |
| **ğŸ’£ Blasting Showdown** | ğŸ–¼ï¸ Image + ğŸ”Š Audio + ğŸ“ State | Multi-agent Bomberman battles | Strategic planning, real-time decision |
| **ğŸ‘» Phantom Soldiers** | ğŸ¬ Video + ğŸ”Š Audio + ğŸ“ Vector | Command units to discover hidden targets | Tactical reasoning, spatial awareness |
| **ğŸ”Š Whispered Pathfinding** | ğŸ–¼ï¸ Image + ğŸ”Š Audio + ğŸ“ Vector | Navigate maze using voice guidance | Audio comprehension, spatial navigation |

<details>
<summary><b>ğŸ“Œ Click to expand game details</b></summary>

### ğŸ§ª The Alchemist's Melody
A sound-based puzzle game where AI must learn the hidden mapping between colors and musical notes, then reproduce increasingly complex sequences.
- **Difficulty Levels**: Easy (4 elements) / Normal (6 elements) / Hard (8 elements)
- **Metrics**: Accuracy, Steps to Complete, Learning Curve
- **Scripts**: `run_mm_agent.py`, `multimodal_agent_baichuan.py`

### ğŸµ Myriad Echoes  
A rhythm memory game requiring observation of icon-sound sequences followed by accurate reproduction.
- **Difficulty Levels**: Level 1-3 (6-15 icons)
- **Metrics**: Sequence Accuracy, Coordinate Precision, Success Rate
- **Scripts**: `eval_openai_multi_episode.py`, `eval_baichuan_multi_episode.py`

### ğŸ’£ Blasting Showdown
Bomberman-style multi-agent competitive game where AI models battle against each other.
- **Features**: Real-time strategy, Multi-AI interaction, Gymnasium environment
- **Metrics**: Kill Count, Survival Rate, Win Rate, Strategy Score
- **Scripts**: `multi_model_game.py`, `start_ai_game.py`

### ğŸ‘» Phantom Soldiers in the Fog
A tactical command game where AI directs hidden units to complete objectives under fog of war.
- **Difficulty Levels**: Normal / Medium / Hard
- **Metrics**: Normalized Score, Objective Completion, Command Compliance
- **Scripts**: `eval-openai-multi-episode.py`, `eval-baichuan-multi-episode.py`

### ğŸ”Š Whispered Pathfinding
A 3D maze navigation game where AI follows voice-based directional guidance to reach the goal.
- **Difficulty Levels**: Easy / Medium / Hard (maze complexity)
- **Metrics**: Success Rate, Path Efficiency, Step Count
- **Scripts**: `test_openai.py`, `test_baichuan.py`

</details>

---

## âš¡ Quick Start

### Installation

```bash
# Clone the repository
git clone https://github.com/fuqingbie/omni-game-benchmark.git
cd omni-game-benchmark

# Install core dependencies
pip install pygame numpy pillow requests gymnasium opencv-python

# Install audio/video processing (required for some games)
pip install moviepy librosa soundfile
```

### Configure API Keys

```bash
# Set environment variables
export OPENAI_API_KEY="your-openai-key"
export BAICHUAN_API_KEY="your-baichuan-key"      # Optional
export DASHSCOPE_API_KEY="your-qwen-key"         # Optional
```

### Run Evaluation

#### Option 1: Unified Framework (Recommended) ğŸš€

```bash
# Run single game with specific model
python eval/game/run_benchmark.py --game alchemist_melody --model gpt-4o --episodes 10

# Run multiple games with config file
python eval/game/run_benchmark.py --config eval/game/benchmark_config.yaml

# List available games and models
python eval/game/run_benchmark.py --list-games
python eval/game/run_benchmark.py --list-models
```

#### Option 2: Individual Game Scripts

```bash
# The Alchemist's Melody
cd eval/game/The_Alchemist-s_Melody && python run_mm_agent.py --episodes 10

# Myriad Echoes
cd eval/game/Myriad_Echoes && python eval_openai_multi_episode.py --num_episodes 10

# Blasting Showdown
cd eval/game/Blasting_Showdown && python multi_model_game.py --config model_config.json

# Phantom Soldiers in the Fog
cd eval/game/Phantom_Soldiers_in_the_Fog && python eval-openai-multi-episode.py --num_episodes 10

# Whispered Pathfinding
cd eval/game/Whispered_Pathfinding && python test_openai.py --difficulty medium --rounds 5
```

---

## ğŸ”§ Unified Framework

OmniPlay provides a unified evaluation framework in `eval/game/common/` for consistent benchmarking.

### Architecture

```
eval/game/
â”œâ”€â”€ common/                      # ğŸ”§ Unified Framework
â”‚   â”œâ”€â”€ model_registry.py        # Model capability management
â”‚   â”œâ”€â”€ video_processor.py       # Video/frame processing modes
â”‚   â”œâ”€â”€ result_schema.py         # Standardized result format
â”‚   â”œâ”€â”€ game_registry.py         # Game registration & discovery
â”‚   â”œâ”€â”€ runner.py                # Batch evaluation runner
â”‚   â””â”€â”€ statistics.py            # Metrics & analysis
â”œâ”€â”€ run_benchmark.py             # ğŸš€ Single entry point
â””â”€â”€ benchmark_config.yaml        # Configuration template
```

### Evaluated Models

| Provider | Model | Video | Audio | Leaderboard Rank |
|:---------|:------|:-----:|:-----:|:----------------:|
| **Google** | Gemini 2.5 Pro | âœ… | âœ… | ğŸ¥‡ 1st |
| **Google** | Gemini 2.5 Flash | âœ… | âœ… | ğŸ¥ˆ 2nd |
| **Baichuan** | Baichuan-Omni-1.5 | âœ… | âœ… | ğŸ¥‰ 3rd |
| **OpenBMB** | MiniCPM-o-2.6 | âœ… | âœ… | 4th |
| **Open** | VITA-1.5 | âœ… | âœ… | 5th |
| **Alibaba** | Qwen-2.5-Omni | âœ… | âœ… | 6th |

### Adding Custom Models

```python
from eval.game.common import get_default_registry

registry = get_default_registry()
registry.register(
    name="my-custom-model",
    api_base="https://api.example.com/v1",
    capability_preset="openai",  # or "baichuan", "qwen", "gemini"
)
```

---

## ğŸ† Leaderboard

> **Last Updated**: October 2025 | [Full Leaderboard â†’](docs/LEADERBOARD.md)

### ğŸ“Š Capability Schism Visualization

<p align="center">
  <img src="docs/assets/radar_chart.png" width="80%" alt="Model Capability Radar Chart"/>
</p>

<p align="center">
  <i>The radar chart reveals a striking "Capability Schism" â€” models excel at audio navigation but struggle with complex multimodal reasoning tasks like tactical command and audio-visual learning.</i>
</p>

### ğŸ¥‡ Overall Ranking

| Rank | Model | Whispered | Echoes | Phantom | Alchemist | Blasting |
|:----:|:------|:---------:|:------:|:-------:|:---------:|:--------:|
| Ref | **Human Expert** | 100.0 | 100.0 | 100.0 | 100.0 | - |
| ğŸ¥‡ | **Gemini 2.5 Pro** | 97.5 | 223.4 | 83.2 | 28.4 | 36.11% |
| ğŸ¥ˆ | **Gemini 2.5 Flash** | 95.5 | 23.7 | 49.1 | 10.5 | 28.95% |
| ğŸ¥‰ | **Baichuan-Omni-1.5** | 88.7 | -2.3 | -3.5 | 10.2 | 17.65% |
| 4 | MiniCPM-o-2.6 | 86.4 | -1.3 | -30.7 | 7.7 | 19.35% |
| 5 | VITA-1.5 | 81.9 | -3.6 | -52.2 | -8.9 | 7.41% |
| 6 | Qwen-2.5-Omni | 73.6 | -2.7 | -7.8 | 9.2 | 11.76% |

> ğŸ“Š *Scores normalized to Human Expert (100.0). Negative = below random baseline. Blasting uses Win Rate %.*

### ğŸ… Per-Game Champions

| Game | Best Model | Score | Insight |
|:-----|:-----------|:-----:|:--------|
| ğŸ”Š Whispered Pathfinding | Gemini 2.5 Pro | 97.5 | Near human-level audio navigation |
| ğŸµ Myriad Echoes | Gemini 2.5 Pro | 223.4 | Exceeds human performance |
| ğŸ‘» Phantom Soldiers | Gemini 2.5 Pro | 83.2 | Best tactical reasoning |
| ğŸ§ª Alchemist's Melody | Gemini 2.5 Pro | 28.4 | Audio-visual learning is hard |
| ğŸ’£ Blasting Showdown | Gemini 2.5 Pro | 36.11% | Highest win rate |

---

## ğŸ“Š Evaluation Metrics

### Universal Metrics

| Metric | Description | Formula |
|:-------|:------------|:--------|
| **Success Rate** | Task completion percentage | `completed / total Ã— 100` |
| **Efficiency** | Steps relative to optimal | `optimal_steps / actual_steps Ã— 100` |
| **Accuracy** | Action/prediction precision | Game-specific |
| **Adaptability** | Cross-difficulty consistency | `std(scores) / mean(scores)` |
| **Multimodal Score** | Modality utilization effectiveness | Weighted combination |

### Overall Score Calculation

$$\text{Score}_{\text{overall}} = \sum_{g \in \text{Games}} w_g \cdot \text{normalize}(\text{metrics}_g)$$

Where weights $w_g$ are based on game complexity and modality diversity.

---

## ğŸ“ Project Structure

```
omni-game-benchmark/
â”œâ”€â”€ eval/game/
â”‚   â”œâ”€â”€ common/                      # ğŸ”§ Unified framework
â”‚   â”‚   â”œâ”€â”€ model_registry.py        # Model capabilities
â”‚   â”‚   â”œâ”€â”€ video_processor.py       # Video processing
â”‚   â”‚   â”œâ”€â”€ result_schema.py         # Result format
â”‚   â”‚   â”œâ”€â”€ game_registry.py         # Game registry
â”‚   â”‚   â”œâ”€â”€ runner.py                # Evaluation runner
â”‚   â”‚   â””â”€â”€ statistics.py            # Statistics
â”‚   â”œâ”€â”€ Blasting_Showdown/           # ğŸ’£ Bomberman game
â”‚   â”œâ”€â”€ The_Alchemist-s_Melody/      # ğŸ§ª Sound puzzle
â”‚   â”œâ”€â”€ Myriad_Echoes/               # ğŸµ Rhythm memory
â”‚   â”œâ”€â”€ Phantom_Soldiers_in_the_Fog/ # ğŸ‘» Tactical command
â”‚   â”œâ”€â”€ Whispered_Pathfinding/       # ğŸ”Š Audio navigation
â”‚   â”œâ”€â”€ assets-necessay/             # Shared assets
â”‚   â”œâ”€â”€ run_benchmark.py             # ğŸš€ Entry point
â”‚   â””â”€â”€ benchmark_config.yaml        # Config template
â”œâ”€â”€ docs/
â”‚   â”œâ”€â”€ LEADERBOARD.md               # Full leaderboard
â”‚   â””â”€â”€ assets/                      # Documentation assets
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ CONTRIBUTING.md
â”œâ”€â”€ LICENSE
â””â”€â”€ README.md
```

---

## âš ï¸ Important Notes

| Item | Recommendation |
|:-----|:---------------|
| **Python Version** | 3.8+ required |
| **RAM** | 8GB+ recommended |
| **GPU** | Optional, speeds up video processing |
| **API Costs** | Monitor usage, especially for video models |
| **Concurrency** | Run one game instance at a time |
| **Audio Libs** | Install `librosa`, `soundfile` for audio games |

---

## ğŸ¤ Contributing

We welcome contributions! See [CONTRIBUTING.md](CONTRIBUTING.md) for guidelines.

### Ways to Contribute

- ğŸ® **Add New Games** â€” Implement `BaseGameEnv` interface
- ğŸ¤– **Add New Models** â€” Register in `model_registry.py`
- ğŸ“Š **Submit Results** â€” Run evaluations and update leaderboard
- ğŸ› **Report Issues** â€” Open GitHub issues
- ğŸ“– **Improve Docs** â€” Enhance documentation

---

## ğŸ“„ License

This project is licensed under the MIT License - see [LICENSE](LICENSE) for details.

---

## ğŸ“– Citation

If you use OmniPlay in your research, please cite our paper:

```bibtex
@misc{bie2025omniplaybenchmarkingomnimodalmodels,
      title={OmniPlay: Benchmarking Omni-Modal Models on Omni-Modal Game Playing}, 
      author={Fuqing Bie and Shiyu Huang and Xijia Tao and Zhiqin Fang and Leyi Pan and Junzhe Chen and Min Ren and Liuyu Xiang and Zhaofeng He},
      year={2025},
      eprint={2508.04361},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2508.04361}, 
}
```

---

<p align="center">
  <b>Made with â¤ï¸ by the OmniPlay Team</b>
  <br>
  <a href="https://github.com/fuqingbie/omni-game-benchmark">GitHub</a> â€¢
  <a href="https://arxiv.org/abs/2508.04361">Paper</a>
</p>

